{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DoC24P6NmAuN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando o Spark"
      ],
      "metadata": {
        "id": "DoC24P6NmAuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NiqW9-xlnjc",
        "outputId": "432f2c16-4a3f-432a-c802-a3807652c786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=84313375126cdcb25ab973bc0b2a595e9da6f2c64af3d467c9ef6d2e497e856f\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark #==3.3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5NeVueloGu2",
        "outputId": "2e08b761-2f93-40c7-bca0-3e5914febca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-12 01:25:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  74.0MB/s    in 0.2s    \n",
            "\n",
            "2023-01-12 01:25:40 (74.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iniciar Sessão Spark"
      ],
      "metadata": {
        "id": "8gIqFurprOZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# ConfigureSparkUI\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder                  # Método da classe que constrói a sessão spark\n",
        "      .appName(\"Meu Primeiro App Spark\")  # Nome do App Spark\n",
        "      .getOrCreate())                     # Verifica se há uma sessão ativa, e se não há, cria uma nova sessão\n"
      ],
      "metadata": {
        "id": "vcytxNzMzrnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels "
      ],
      "metadata": {
        "id": "tRgaHosSo6PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL"
      ],
      "metadata": {
        "id": "9mPHkAtuYNO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "\n",
        "caminho_csv = \"./base_de_dados.csv\"\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('parte_debitada_nome', StringType()),\n",
        "    StructField('parte_debitada_conta', StringType()),\n",
        "    StructField('parte_debitada_banco', StringType()),\n",
        "    StructField('parte_creditada_nome', StringType()),\n",
        "    StructField('parte_creditada_conta', StringType()),\n",
        "    StructField('parte_creditada_banco', StringType()),\n",
        "    StructField('chave_pix_tipo', StringType()),\n",
        "    StructField('chave_pix_valor', StringType()),\n",
        "    StructField('data_transacao', TimestampType())\n",
        "])\n",
        "\n",
        "df = spark.read.csv(\n",
        "    path=caminho_csv,\n",
        "    header=True,\n",
        "    sep=\";\",\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
        ")\n",
        "spark.read.csv(\n",
        "    path=caminho_csv,\n",
        "    header=True,\n",
        "    sep=\";\",\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
        ").createOrReplaceTempView(\"base_pix\")"
      ],
      "metadata": {
        "id": "Tww10QvPhTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from base_pix limit 3\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM-eqPLQuT23",
        "outputId": "93899c68-ccf4-4fbc-fb9b-d235a56454fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "| id|valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|     data_transacao|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "|  1| 9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|2022-02-18 13:28:00|\n",
            "|  2|15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|2022-04-08 01:47:00|\n",
            "|  3|57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|2022-07-14 03:18:00|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porém, como saber se a manipulação de dados com Dataframes não é mais rápida que SQL?\n",
        "\n",
        "Para isso vamos propor um group by das duas maneiras e verificar qual é o plano de execução que o spark cria. \n"
      ],
      "metadata": {
        "id": "GYLebBzY2dWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql = spark.sql(\"select chave_pix_tipo, count(1) from base_pix group by chave_pix_tipo\")"
      ],
      "metadata": {
        "id": "mz1XUVMb17_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_dataframe = df.groupBy('chave_pix_tipo').count()"
      ],
      "metadata": {
        "id": "GvnpVoI92Jlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2_i9k40-lush"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SQL Group\")\n",
        "group_sql.explain()\n",
        "\n",
        "print(\"DataFrame Group\")\n",
        "group_dataframe.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB_IsUNh2St3",
        "outputId": "2043473d-e0c7-459c-b2c0-434b61bd6925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL Group\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix_tipo#123], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix_tipo#123, 200), ENSURE_REQUIREMENTS, [plan_id=97]\n",
            "      +- HashAggregate(keys=[chave_pix_tipo#123], functions=[partial_count(1)])\n",
            "         +- FileScan csv [chave_pix_tipo#123] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/base_de_dados.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
            "\n",
            "\n",
            "DataFrame Group\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix_tipo#101], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix_tipo#101, 200), ENSURE_REQUIREMENTS, [plan_id=110]\n",
            "      +- HashAggregate(keys=[chave_pix_tipo#101], functions=[partial_count(1)])\n",
            "         +- FileScan csv [chave_pix_tipo#101] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/base_de_dados.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, sum(valor) \n",
        "    from base_pix \n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_1T-ov8mKoP",
        "outputId": "de92dfa2-4f9e-4f1c-cc38-b8d8ea79aaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+\n",
            "|chave_pix_tipo|        sum(valor)|\n",
            "+--------------+------------------+\n",
            "|       celular|         207778.46|\n",
            "|         email|499009.38000000006|\n",
            "|           cpf| 659513.3499999997|\n",
            "+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, round(sum(valor), 2)\n",
        "    from base_pix \n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkFXNzwhmrZb",
        "outputId": "4208078d-8c85-4a39-ff3d-cc3abf1df83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+\n",
            "|chave_pix_tipo|round(sum(valor), 2)|\n",
            "+--------------+--------------------+\n",
            "|       celular|           207778.46|\n",
            "|         email|           499009.38|\n",
            "|           cpf|           659513.35|\n",
            "+--------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, round(sum(valor), 2) as sum_valor\n",
        "    from base_pix \n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmmLdhNym0pL",
        "outputId": "f6a18e5e-12c3-4786-ee4f-df0754158dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------+\n",
            "|chave_pix_tipo|sum_valor|\n",
            "+--------------+---------+\n",
            "|       celular|207778.46|\n",
            "|         email|499009.38|\n",
            "|           cpf|659513.35|\n",
            "+--------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, count(*) as count\n",
        "    from base_pix \n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPBMgMZ-m3E_",
        "outputId": "ea17a408-3c4c-400d-f743-01426ebc1dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|chave_pix_tipo|count|\n",
            "+--------------+-----+\n",
            "|       celular|   22|\n",
            "|         email|   29|\n",
            "|           cpf|   49|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sYTJLvr0Sq-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARA AQUI"
      ],
      "metadata": {
        "id": "Bycg9Xv2SrD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "      select\n",
        "        destinatario.banco,\n",
        "        valor,\n",
        "        row_number() over (partition by destinatario.banco order by valor desc) as row_number\n",
        "      from transacoes_pix\n",
        "      limit 10\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcBybrlenM0e",
        "outputId": "7561c4c5-d587-4dae-91f1-0af29ad3bc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+---------+\n",
            "|parte_creditada_banco|     data_transacao|RowNumber|\n",
            "+---------------------+-------------------+---------+\n",
            "|                  BTG|2022-12-08 23:47:00|        1|\n",
            "|                  BTG|2022-11-29 06:17:00|        2|\n",
            "|                  BTG|2022-11-04 01:50:00|        3|\n",
            "|                  BTG|2022-08-26 00:56:00|        4|\n",
            "|                  BTG|2022-08-24 15:39:00|        5|\n",
            "|                  BTG|2022-08-04 00:48:00|        6|\n",
            "|                  BTG|2022-07-30 19:56:00|        7|\n",
            "|                  BTG|2022-07-18 22:46:00|        8|\n",
            "|                  BTG|2022-07-14 03:18:00|        9|\n",
            "|                  BTG|2022-07-03 23:37:00|       10|\n",
            "|                  BTG|2022-06-05 12:14:00|       11|\n",
            "|                  BTG|2022-05-23 06:48:00|       12|\n",
            "|                  BTG|2022-05-06 11:33:00|       13|\n",
            "|                  BTG|2022-04-28 16:47:00|       14|\n",
            "|                  BTG|2022-02-26 15:05:00|       15|\n",
            "|                  BTG|2022-02-21 11:25:00|       16|\n",
            "|                  BTG|2021-12-14 13:54:00|       17|\n",
            "|                  BTG|2021-12-09 06:10:00|       18|\n",
            "|                  BTG|2021-11-27 19:30:00|       19|\n",
            "|                  BTG|2021-11-21 09:17:00|       20|\n",
            "+---------------------+-------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CTE stands for common table expression. A CTE allows you to define a temporary named result set that available temporarily in the execution scope of a statement such as SELECT, INSERT, UPDATE, DELETE, or MERGE"
      ],
      "metadata": {
        "id": "dpfsHISapCwS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0E37n56ybULQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "  with base_pix_row_number as(\n",
        "    select\n",
        "      parte_creditada_banco, \n",
        "      data_transacao,\n",
        "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
        "    from base_pix\n",
        "  ) select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao\n",
        "    from base_pix_row_number\n",
        "    where row_number = 1\n",
        "    order by data_transacao desc\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu8jGKlKoJoq",
        "outputId": "b45492ea-c20a-4897-c26e-197497364f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+\n",
            "|parte_creditada_banco|     data_transacao|\n",
            "+---------------------+-------------------+\n",
            "|                 Itau|2022-12-15 01:29:00|\n",
            "|                  BTG|2022-12-08 23:47:00|\n",
            "|               Nubank|2022-11-19 19:25:00|\n",
            "|             Bradesco|2022-08-07 17:01:00|\n",
            "+---------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porém, não precisa ficar limitado somente a execução de queries SQL. \n",
        "\n",
        "Podemos pegar o resultado de uma query e retorná-la para um DataFrame!"
      ],
      "metadata": {
        "id": "lkSb46FNp3Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_window = spark.sql(\n",
        "  \"\"\"\n",
        "  with base_pix_row_number as(\n",
        "    select\n",
        "      parte_creditada_banco, \n",
        "      data_transacao,\n",
        "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
        "    from base_pix\n",
        "  ) select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao\n",
        "    from base_pix_row_number\n",
        "    where row_number = 1\n",
        "    order by data_transacao desc\n",
        "  \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "h5vx27b8pxb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_window.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JosM2a0Bp1Cq",
        "outputId": "068b683a-4a86-419f-8eae-41d75ff704d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+\n",
            "|parte_creditada_banco|     data_transacao|\n",
            "+---------------------+-------------------+\n",
            "|                 Itau|2022-12-15 01:29:00|\n",
            "|                  BTG|2022-12-08 23:47:00|\n",
            "|               Nubank|2022-11-19 19:25:00|\n",
            "|             Bradesco|2022-08-07 17:01:00|\n",
            "+---------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This opens up the true power of Spark. We can treat selectExpr as a simple way to build up\n",
        "complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating\n",
        "SQL statement, and as long as the columns resolve, it will be valid!"
      ],
      "metadata": {
        "id": "G7DBXmibq97j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "oq-ec7scstUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.selectExpr(\n",
        "    \"date(data_transacao) as date_data_transacao\",\n",
        "    \"va\"\n",
        ").groupBy('date_data_transacao').count().orderBy(col('count').desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eri84Mkgq1fK",
        "outputId": "e8452864-b344-4124-c7f7-1001cc0969e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|date_data_transacao|count|\n",
            "+-------------------+-----+\n",
            "|         2022-02-26|    2|\n",
            "|         2022-03-02|    2|\n",
            "|         2021-06-22|    1|\n",
            "|         2022-11-29|    1|\n",
            "|         2021-02-15|    1|\n",
            "|         2022-02-16|    1|\n",
            "|         2021-07-20|    1|\n",
            "|         2022-01-09|    1|\n",
            "|         2021-03-22|    1|\n",
            "|         2022-04-12|    1|\n",
            "|         2021-04-25|    1|\n",
            "|         2021-03-07|    1|\n",
            "|         2022-01-15|    1|\n",
            "|         2022-05-23|    1|\n",
            "|         2022-02-01|    1|\n",
            "|         2021-07-11|    1|\n",
            "|         2022-06-05|    1|\n",
            "|         2021-09-06|    1|\n",
            "|         2021-06-20|    1|\n",
            "|         2021-12-14|    1|\n",
            "+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercício\n",
        "1. Vimos que há dois dias em que houve duas transações pix. Descubra são os ids dessas transações.\n",
        "\n",
        "2. Vimos que há dois dias em que houve duas transações pix. Descubra quais chaves pix foram utilizadas para realizar as transações. "
      ],
      "metadata": {
        "id": "y-ob-JQ_tPvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_datas = spark.sql(\n",
        "  \"\"\"\n",
        "  select\n",
        "    date(data_transacao) as date_data_transacao\n",
        "  from base_pix\n",
        "  group by 1\n",
        "  having count(*) > 1\n",
        "  \"\"\"\n",
        ").collect()[0][0]\n",
        "lista_datas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbKknPUysbcK",
        "outputId": "ec74de15-2b40-455a-c9bb-421e0418e50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2022, 2, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}